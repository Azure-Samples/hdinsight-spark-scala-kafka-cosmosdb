{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stream data to from Kafka to Cosmos DB\n",
    "\n",
    "This notebook uses Spark Structured Streaming to retrieve data from Kafka on HDInsight and store it into Azure Cosmos DB. It uses the [Azure CosmosDB Spark Connector](https://github.com/Azure/azure-cosmosdb-spark) to write to a Cosmos DB SQL API database. For more information on using the connector, see [https://github.com/Azure/azure-cosmosdb-spark](https://github.com/Azure/azure-cosmosdb-spark)\n",
    "\n",
    "## To use this notebook\n",
    "\n",
    "Jupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n",
    "\n",
    "NOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* An Azure Virtual Network\n",
    "* A Spark on HDInsight 4.0 cluster, inside the virtual network\n",
    "* A Kafka on HDInsight 4.0 cluster, inside the virtual network\n",
    "* A Cosmos DB SQL API database\n",
    "\n",
    "## Load packages\n",
    "\n",
    "Run the next cell to load the packages used by this notebook:\n",
    "\n",
    "* spark-sql-kafka-0-10_2.11, version 2.2.0 - Used to read from Kafka.\n",
    "* azure-cosmosdb-spark_2.4.0_2.11, version 3.7.0 - The Spark connector used to communicate with Azure Cosmos DB.\n",
    "* azure-documentdb, version 2.6.4 - The DocumentDB SDK. This is used by the connector to communicate with Cosmos DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"name\":\"Spark-to-Cosmos_DB_Connector\", \n",
    "    \"executorMemory\": \"8G\", \n",
    "    \"executorCores\": 2, \n",
    "    \"numExecutors\":9,\n",
    "    \"driverMemory\" : \"2G\",\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.microsoft.azure:azure-cosmosdb-spark_2.4.0_2.11:3.7.0,com.microsoft.azure:azure-documentdb:2.6.4\", \n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set the Kafka broker hosts information\n",
    "\n",
    "In the next cell, replace YOUR_KAFKA_BROKER_HOSTS with the broker hosts for your Kafka cluster. This is used to write data to the Kafka cluster. To get the broker host information, use one of the following methods:\n",
    "\n",
    "* From Bash or other Unix shell:\n",
    "\n",
    "    ```bash\n",
    "CLUSTERNAME='the name of your HDInsight cluster'\n",
    "PASSWORD='the password for your cluster login account'\n",
    "curl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2\n",
    "    ```\n",
    "\n",
    "* From Azure Powershell:\n",
    "\n",
    "    ```powershell\n",
    "$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n",
    "$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n",
    "$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/KAFKA/components/KAFKA_BROKER\" `\n",
    "  -Credential $creds `\n",
    "  -UseBasicParsing\n",
    "$respObj = ConvertFrom-Json $resp.Content\n",
    "$brokerHosts = $respObj.host_components.HostRoles.host_name[0..1]\n",
    "($brokerHosts -join \":9092,\") + \":9092\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// The Kafka broker hosts and topic used to read to Kafka\n",
    "val kafkaBrokers=\"YOUR_BROKER_HOSTS\"\n",
    "val kafkaTopic=\"tripdata\"\n",
    "\n",
    "println(\"broker and topic set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Configure the Cosmos DB connection information\n",
    "\n",
    "In the following cell, you must provide the information used to connect to your Cosmos DB. Use the information in [Create a document database using Java and the Azure portal](https://docs.microsoft.com/en-us/azure/cosmos-db/create-sql-api-java) to create a database and collection, then retrieve the endpoint, master key, and preferred region information.\n",
    "\n",
    "__NOTE__: When following the steps in [Create a document database using Java and the Azure portal](https://docs.microsoft.com/en-us/azure/cosmos-db/create-sql-api-java), you do not need to add sample data to the collection or build the code. You only need to create the database, collection, and retrieve the connection information.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Import Necessary Libraries\n",
    "import org.joda.time._\n",
    "import org.joda.time.format._\n",
    "\n",
    "// Current version of the connector\n",
    "import com.microsoft.azure.cosmosdb.spark.schema._\n",
    "import com.microsoft.azure.cosmosdb.spark._\n",
    "import com.microsoft.azure.cosmosdb.spark.streaming.CosmosDBSinkProvider\n",
    "import com.microsoft.azure.cosmosdb.spark.config.Config\n",
    "\n",
    "var configMap = Map(\n",
    "    \"Endpoint\" -> \"YOUR_COSMOSDB_ENDPOINT\",\n",
    "    \"Masterkey\" -> \"YOUR_MASTER_KEY\",\n",
    "    \"Database\" -> \"kafkadata\",\n",
    "    // use a ';' to delimit multiple regions\n",
    "    \"PreferredRegions\" -> \"West US;\",\n",
    "    \"Collection\" -> \"kafkacollection\"\n",
    ")\n",
    "\n",
    "println(\"Cosmos DB configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define the schema and source stream\n",
    "\n",
    "The following cell creates the stream that reads from Kafka. Data read from Kafka contains several columns. In this case, we only use the `value` column, as it contains the taxi trip data written by the other notebook. To make this data easier to work with, a schema is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Import bits useed for declaring schemas and working with JSON data\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Define a schema for the data\n",
    "val schema = (new StructType).add(\"dropoff_latitude\", StringType).add(\"dropoff_longitude\", StringType).add(\"extra\", StringType).add(\"fare_amount\", StringType).add(\"improvement_surcharge\", StringType).add(\"lpep_dropoff_datetime\", StringType).add(\"lpep_pickup_datetime\", StringType).add(\"mta_tax\", StringType).add(\"passenger_count\", StringType).add(\"payment_type\", StringType).add(\"pickup_latitude\", StringType).add(\"pickup_longitude\", StringType).add(\"ratecodeid\", StringType).add(\"store_and_fwd_flag\", StringType).add(\"tip_amount\", StringType).add(\"tolls_amount\", StringType).add(\"total_amount\", StringType).add(\"trip_distance\", StringType).add(\"trip_type\", StringType).add(\"vendorid\", StringType)\n",
    "// Reproduced here for readability\n",
    "//val schema = (new StructType)\n",
    "//   .add(\"dropoff_latitude\", StringType)\n",
    "//   .add(\"dropoff_longitude\", StringType)\n",
    "//   .add(\"extra\", StringType)\n",
    "//   .add(\"fare_amount\", StringType)\n",
    "//   .add(\"improvement_surcharge\", StringType)\n",
    "//   .add(\"lpep_dropoff_datetime\", StringType)\n",
    "//   .add(\"lpep_pickup_datetime\", StringType)\n",
    "//   .add(\"mta_tax\", StringType)\n",
    "//   .add(\"passenger_count\", StringType)\n",
    "//   .add(\"payment_type\", StringType)\n",
    "//   .add(\"pickup_latitude\", StringType)\n",
    "//   .add(\"pickup_longitude\", StringType)\n",
    "//   .add(\"ratecodeid\", StringType)\n",
    "//   .add(\"store_and_fwd_flag\", StringType)\n",
    "//   .add(\"tip_amount\", StringType)\n",
    "//   .add(\"tolls_amount\", StringType)\n",
    "//   .add(\"total_amount\", StringType)\n",
    "//   .add(\"trip_distance\", StringType)\n",
    "//   .add(\"trip_type\", StringType)\n",
    "//   .add(\"vendorid\", StringType)\n",
    "\n",
    "// Read from the Kafka stream source\n",
    "val kafka = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"subscribe\", kafkaTopic).option(\"startingOffsets\",\"earliest\").load()\n",
    "\n",
    "// Select the value of the Kafka message and apply the trip schema to it\n",
    "val taxiData = kafka.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema) as \"trip\")\n",
    "\n",
    "// The output of this cell is similar to the following value:\n",
    "// taxiData: org.apache.spark.sql.DataFrame = [trip: struct<dropoff_latitude: string, dropoff_longitude: string ... 18 more fields>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Write the data to Cosmos DB\n",
    "\n",
    "The following cell selects the trip data from the stream and writes it to Cosmos DB. This is the data structure that was created in the previous cell by applying a schema to the value data retrieved from kafka.\n",
    "\n",
    "This stream only runs for 10 seconds (10000ms). Please make sure that the Stream-taxi-data-to-Kafka notebook is actively streaming data into Kafka during this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val query = taxiData.select(\"trip\").\n",
    "            writeStream.\n",
    "            foreachBatch { (batchDF: DataFrame, batchId: Long) =>\n",
    "              batchDF.persist()\n",
    "              batchDF.write.format(\"com.microsoft.azure.cosmosdb.spark\").\n",
    "                options(configMap).\n",
    "                option(\"spark.cosmos.write.upsertEnabled\", \"true\").\n",
    "                mode(SaveMode.Overwrite).\n",
    "                save()\n",
    "              println(s\"BatchId: $batchId, Document count: ${batchDF.count()}\")\n",
    "              batchDF.unpersist()\n",
    "              ()\n",
    "            }.        \n",
    "            option(\"checkpointLocation\", \"cosmoscheckpointlocation\").\n",
    "            start()\n",
    "query.awaitTermination(10000)\n",
    "println(\"Stream finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## To verify that data is in Cosmos DB\n",
    "\n",
    "In the [Azure portal](https://portal.azure.com), select your Cosmos DB account, and then select __Document Explorer__. From the dropdown, select the database and collection that the data is written to. You may need to select __Refresh__ before the data appears. Select the id of one of the entries to view the data in Cosmos DB. The document should contain data similar to the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"trip\": {\n",
    "    \"fare_amount\": \"14.5\",\n",
    "    \"pickup_longitude\": \"-73.988777160644531\",\n",
    "    \"lpep_dropoff_datetime\": \"2016-01-01T00:43:11.000\",\n",
    "    \"lpep_pickup_datetime\": \"2016-01-01T00:28:24.000\",\n",
    "    \"passenger_count\": \"2\",\n",
    "    \"vendorid\": \"2\",\n",
    "    \"tolls_amount\": \"0\",\n",
    "    \"dropoff_latitude\": \"40.729816436767578\",\n",
    "    \"improvement_surcharge\": \"0.3\",\n",
    "    \"trip_distance\": \"3.66\",\n",
    "    \"dropoff_longitude\": \"-73.996437072753906\",\n",
    "    \"payment_type\": \"2\",\n",
    "    \"store_and_fwd_flag\": \"N\",\n",
    "    \"trip_type\": \"1\",\n",
    "    \"ratecodeid\": \"1\",\n",
    "    \"total_amount\": \"15.8\",\n",
    "    \"pickup_latitude\": \"40.690895080566406\",\n",
    "    \"extra\": \"0.5\",\n",
    "    \"tip_amount\": \"0\",\n",
    "    \"mta_tax\": \"0.5\"\n",
    "  },\n",
    "  \"id\": \"abfe6ff1-51a7-46a6-9600-1c330166cf12\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark | Scala",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
