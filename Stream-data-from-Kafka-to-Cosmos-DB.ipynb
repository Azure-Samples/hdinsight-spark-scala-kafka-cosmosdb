{"nbformat_minor": 2, "cells": [{"source": "# Stream data to from Kafka to Cosmos DB\n\nThis notebook uses Spark Structured Streaming to retrieve data from Kafka on HDInsight and store it into Azure Cosmos DB. It uses the [Azure CosmosDB Spark Connector](https://github.com/Azure/azure-cosmosdb-spark) to write to a Cosmos DB SQL API database. For more information on using the connector, see [https://github.com/Azure/azure-cosmosdb-spark](https://github.com/Azure/azure-cosmosdb-spark)\n\n## To use this notebook\n\nJupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n\nNOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.\n\n## Requirements\n\n* An Azure Virtual Network\n* A Spark on HDInsight 3.6 cluster, inside the virtual network\n* A Kafka on HDInsight cluster, inside the virtual network\n* A Cosmos DB SQL API database\n\n## Load packages\n\nRun the next cell to load the packages used by this notebook:\n\n* spark-sql-kafka-0-10_2.11, version 2.1.0 - Used to read from Kafka.\n* azure-cosmosdb-spark_2.1.0_2.11, version 1.0.0 - The Spark connector used to communicate with Azure Cosmos DB.\n* azure-documentdb, version 1.15.1 - The DocumentDB SDK. This is used by the connector to communicate with Cosmos DB.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\n    \"name\":\"Spark-to-Cosmos_DB_Connector\", \n    \"executorMemory\": \"8G\", \n    \"executorCores\": 2, \n    \"numExecutors\":9,\n    \"driverMemory\" : \"2G\",\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.microsoft.azure:azure-cosmosdb-spark_2.2.0_2.11:1.0.0,com.microsoft.azure:azure-documentdb:1.15.1\", \n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'name': u'Spark-to-Cosmos_DB_Connector', u'driverMemory': u'2G', u'numExecutors': 9, u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.microsoft.azure:azure-cosmosdb-spark_2.2.0_2.11:1.0.0,com.microsoft.azure:azure-documentdb:1.15.1', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}, u'executorCores': 2, u'executorMemory': u'8G'}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1533323783391_0006</td><td>spark</td><td>busy</td><td><a target=\"_blank\" href=\"http://hn1-spark.d4lt02jpu35epdgcvwdc51hehh.cx.internal.cloudapp.net:8088/proxy/application_1533323783391_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-spark.d4lt02jpu35epdgcvwdc51hehh.cx.internal.cloudapp.net:30060/node/containerlogs/container_1533323783391_0006_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "## Set the Kafka broker hosts information\n\nIn the next cell, replace YOUR_KAFKA_BROKER_HOSTS with the broker hosts for your Kafka cluster. This is used to write data to the Kafka cluster. To get the broker host information, use one of the following methods:\n\n* From Bash or other Unix shell:\n\n    ```bash\nCLUSTERNAME='the name of your HDInsight cluster'\nPASSWORD='the password for your cluster login account'\ncurl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2\n    ```\n\n* From Azure Powershell:\n\n    ```powershell\n$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/KAFKA/components/KAFKA_BROKER\" `\n  -Credential $creds `\n  -UseBasicParsing\n$respObj = ConvertFrom-Json $resp.Content\n$brokerHosts = $respObj.host_components.HostRoles.host_name[0..1]\n($brokerHosts -join \":9092,\") + \":9092\"\n    ```", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to read to Kafka\nval kafkaBrokers=\"YOUR_BROKER_HOSTS\"\nval kafkaTopic=\"tripdata\"\n\nprintln(\"broker and topic set.\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1531345171959_0013</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark.jcc2uw4e5n3ufjhwet5exykice.dx.internal.cloudapp.net:8088/proxy/application_1531345171959_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-spark.jcc2uw4e5n3ufjhwet5exykice.dx.internal.cloudapp.net:30060/node/containerlogs/container_1531345171959_0013_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nbroker and topic set."}], "metadata": {"collapsed": false}}, {"source": "## Configure the Cosmos DB connection information\n\nIn the following cell, you must provide the information used to connect to your Cosmos DB. Use the information in [Create a document database using Java and the Azure portal](https://docs.microsoft.com/en-us/azure/cosmos-db/create-sql-api-java) to create a database and collection, then retrieve the endpoint, master key, and preferred region information.\n\n__NOTE__: When following the steps in [Create a document database using Java and the Azure portal](https://docs.microsoft.com/en-us/azure/cosmos-db/create-sql-api-java), you do not need to add sample data to the collection or build the code. You only need to create the database, collection, and retrieve the connection information.\n    ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "// Import Necessary Libraries\nimport org.joda.time._\nimport org.joda.time.format._\n\n// Current version of the connector\nimport com.microsoft.azure.cosmosdb.spark.schema._\nimport com.microsoft.azure.cosmosdb.spark._\nimport com.microsoft.azure.cosmosdb.spark.streaming.CosmosDBSinkProvider\nimport com.microsoft.azure.cosmosdb.spark.config.Config\n\nvar configMap = Map(\n    \"Endpoint\" -> \"YOUR_COSMOSDB_ENDPOINT\",\n    \"Masterkey\" -> \"YOUR_MASTER_KEY\",\n    \"Database\" -> \"kafkadata\",\n    // use a ';' to delimit multiple regions\n    \"PreferredRegions\" -> \"West US;\",\n    \"Collection\" -> \"kafkacollection\"\n)\n\nprintln(\"Cosmos DB configuration set.\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Cosmos DB configuration set."}], "metadata": {"collapsed": false}}, {"source": "## Define the schema and source stream\n\nThe following cell creates the stream that reads from Kafka. Data read from Kafka contains several columns. In this case, we only use the `value` column, as it contains the taxi trip data written by the other notebook. To make this data easier to work with, a schema is applied.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "// Import bits useed for declaring schemas and working with JSON data\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\n// Define a schema for the data\nval schema = (new StructType).add(\"dropoff_latitude\", StringType).add(\"dropoff_longitude\", StringType).add(\"extra\", StringType).add(\"fare_amount\", StringType).add(\"improvement_surcharge\", StringType).add(\"lpep_dropoff_datetime\", StringType).add(\"lpep_pickup_datetime\", StringType).add(\"mta_tax\", StringType).add(\"passenger_count\", StringType).add(\"payment_type\", StringType).add(\"pickup_latitude\", StringType).add(\"pickup_longitude\", StringType).add(\"ratecodeid\", StringType).add(\"store_and_fwd_flag\", StringType).add(\"tip_amount\", StringType).add(\"tolls_amount\", StringType).add(\"total_amount\", StringType).add(\"trip_distance\", StringType).add(\"trip_type\", StringType).add(\"vendorid\", StringType)\n// Reproduced here for readability\n//val schema = (new StructType)\n//   .add(\"dropoff_latitude\", StringType)\n//   .add(\"dropoff_longitude\", StringType)\n//   .add(\"extra\", StringType)\n//   .add(\"fare_amount\", StringType)\n//   .add(\"improvement_surcharge\", StringType)\n//   .add(\"lpep_dropoff_datetime\", StringType)\n//   .add(\"lpep_pickup_datetime\", StringType)\n//   .add(\"mta_tax\", StringType)\n//   .add(\"passenger_count\", StringType)\n//   .add(\"payment_type\", StringType)\n//   .add(\"pickup_latitude\", StringType)\n//   .add(\"pickup_longitude\", StringType)\n//   .add(\"ratecodeid\", StringType)\n//   .add(\"store_and_fwd_flag\", StringType)\n//   .add(\"tip_amount\", StringType)\n//   .add(\"tolls_amount\", StringType)\n//   .add(\"total_amount\", StringType)\n//   .add(\"trip_distance\", StringType)\n//   .add(\"trip_type\", StringType)\n//   .add(\"vendorid\", StringType)\n\n// Read from the Kafka stream source\nval kafka = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafkaBrokers).option(\"subscribe\", kafkaTopic).option(\"startingOffsets\",\"earliest\").load()\n\n// Select the value of the Kafka message and apply the trip schema to it\nval taxiData = kafka.select(\n    from_json(col(\"value\").cast(\"string\"), schema) as \"trip\")\n\n// The output of this cell is similar to the following value:\n// taxiData: org.apache.spark.sql.DataFrame = [trip: struct<dropoff_latitude: string, dropoff_longitude: string ... 18 more fields>]", "outputs": [{"output_type": "stream", "name": "stdout", "text": "taxiData: org.apache.spark.sql.DataFrame = [trip: struct<dropoff_latitude: string, dropoff_longitude: string ... 18 more fields>]"}], "metadata": {"collapsed": false}}, {"source": "## Write the data to Cosmos DB\n\nThe following cell selects the trip data from the stream and writes it to Cosmos DB. This is the data structure that was created in the previous cell by applying a schema to the value data retrieved from kafka.\n\nThis stream only runs for 10 seconds (10000ms). Please make sure that the Stream-taxi-data-to-Kafka notebook is actively streaming data into Kafka during this time.", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "taxiData.select(\"trip\").writeStream.format(classOf[CosmosDBSinkProvider].getName).outputMode(\"append\").options(configMap).option(\"checkpointLocation\", \"cosmoscheckpointlocation\").start.awaitTermination(10000)\nprintln(\"Stream finished.\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Stream finished."}], "metadata": {"collapsed": false}}, {"source": "## To verify that data is in Cosmos DB\n\nIn the [Azure portal](https://portal.azure.com), select your Cosmos DB account, and then select __Document Explorer__. From the dropdown, select the database and collection that the data is written to. You may need to select __Refresh__ before the data appears. Select the id of one of the entries to view the data in Cosmos DB. The document should contain data similar to the following:\n\n```json\n{\n  \"trip\": {\n    \"fare_amount\": \"14.5\",\n    \"pickup_longitude\": \"-73.988777160644531\",\n    \"lpep_dropoff_datetime\": \"2016-01-01T00:43:11.000\",\n    \"lpep_pickup_datetime\": \"2016-01-01T00:28:24.000\",\n    \"passenger_count\": \"2\",\n    \"vendorid\": \"2\",\n    \"tolls_amount\": \"0\",\n    \"dropoff_latitude\": \"40.729816436767578\",\n    \"improvement_surcharge\": \"0.3\",\n    \"trip_distance\": \"3.66\",\n    \"dropoff_longitude\": \"-73.996437072753906\",\n    \"payment_type\": \"2\",\n    \"store_and_fwd_flag\": \"N\",\n    \"trip_type\": \"1\",\n    \"ratecodeid\": \"1\",\n    \"total_amount\": \"15.8\",\n    \"pickup_latitude\": \"40.690895080566406\",\n    \"extra\": \"0.5\",\n    \"tip_amount\": \"0\",\n    \"mta_tax\": \"0.5\"\n  },\n  \"id\": \"abfe6ff1-51a7-46a6-9600-1c330166cf12\"\n}\n```", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}