{"nbformat_minor": 2, "cells": [{"source": "# Retrive taxi trip data for New York city\n\nThis notebook retrieves data on taxi trips, whiuch is provided by the New York City. The data is stored into Kafka, and then used by the other notebook in this project (Stream-data-from-Kafka-to-Cosmos-DB) to demonstrate how to write data into Cosmos.\n\nThe data set used by this notebook is from [2016 Green Taxi Trip Data](https://data.cityofnewyork.us/Transportation/2016-Green-Taxi-Trip-Data/hvrh-b6nb).\n\n## To use this notebook\n\nJupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n\nNOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.\n\n## Requirements\n\n* An Azure Virtual Network\n* A Spark (2.2.0) on HDInsight 3.6 cluster, inside the virtual network\n* A Kafka on HDInsight 3.6 cluster, inside the virtual network\n\n## Load packages\n\nRun the next cell to load packages used by this notebook:\n\n* spark-streaming-kafka-0-8_2.10, version 2.2.0 - Used to write data to Kafka.\n* gson version 2.4 - Used for JSON parsing.\n\n__NOTE__: The first time you run this block, it may take a minute or longer. This happens because the Spark cluster must retrieve the packages from the Maven repository on the internet.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-streaming-kafka-0-8_2.10:2.2.0,com.google.code.gson:gson:2.4\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Create the Kafka topic\n\nIn the next cell, you must provide the Zookeeper host information for your Kafka cluster. Use the following steps to get this information:\n\n* From __Bash__ or other Unix shell:\n\n    ```bash\nCLUSTERNAME='the name of your HDInsight cluster'\nPASSWORD='the password for your cluster login account'\ncurl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):2181\"] | join(\",\")' | cut -d',' -f1,2\n    ```\n\n* From __Azure PowerShell__:\n\n    ```powershell\n$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" `\n    -Credential $creds `\n    -UseBasicParsing\n$respObj = ConvertFrom-Json $resp.Content\n$zkHosts = $respObj.host_components.HostRoles.host_name[0..1]\n($zkHosts -join \":2181,\") + \":2181\"\n    ````\n\nThe return value is similar to the following example:\n\n`zk0-kafka.ztgnbfvxu2mudoa5h5zzc1uncg.cx.internal.cloudapp.net:2181,zk1-kafka.ztgnbfvxu2mudoa5h5zzc1uncg.cx.internal.cloudapp.net:2181`\n\nReplace the `YOUR_ZOOKEEPER_HOSTS` in the next cell with the returned value, and then run the cell", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash \n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic tripdata --zookeeper 'YOUR_ZOOKEEPER_HOSTS'", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Retrieve data on taxi trips\n\nRun the next cell to load data on taxi trips in New York City.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Load the data from the New York City Taxi data REST API for 2016 Green Taxi Trip Data\nval url=\"https://data.cityofnewyork.us/resource/pqfs-mqru.json\"\nval result = scala.io.Source.fromURL(url).mkString\n\n// Since the REST API returns an array of items,\n// it's easier to use as an array than deal with streaming\nimport com.google.gson.Gson\nval gson = new Gson()\nval jsonDataArray = gson.fromJson(result, classOf[Array[Object]])\n\nprintln(\"Retrieved \" + jsonDataArray.length + \" rows of Taxi data.\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Set the Kafka broker hosts information\n\nIn the next cell, replace YOUR_KAFKA_BROKER_HOSTS with the broker hosts for your Kafka cluster. This is used to write data to the Kafka cluster. To get the broker host information, use one of the following methods:\n\n* From Bash or other Unix shell:\n\n    ```bash\nCLUSTERNAME='the name of your HDInsight cluster'\nPASSWORD='the password for your cluster login account'\ncurl -u admin:$PASSWORD -G \"https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER\" | jq -r '[\"\\(.host_components[].HostRoles.host_name):9092\"] | join(\",\")' | cut -d',' -f1,2\n    ```\n\n* From Azure Powershell:\n\n    ```powershell\n$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/KAFKA/components/KAFKA_BROKER\" `\n  -Credential $creds `\n  -UseBasicParsing\n$respObj = ConvertFrom-Json $resp.Content\n$brokerHosts = $respObj.host_components.HostRoles.host_name[0..1]\n($brokerHosts -join \":9092,\") + \":9092\"\n    ```", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to write to Kafka\nval kafkaBrokers=\"YOUR_BROKER_HOSTS\"\nval kafkaTopic=\"tripdata\"", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Send the data to Kafka\n\nRun the following cell to begin streaming data to Kafka. There is a delay of 1 second (1000ms) after each send, so this cell will stay active several minutes. This provides you the time needed to load the other notebook and run the cells in it while data is flowing into Kafka.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "// Import classes used to write to Kafka via a producer\nimport org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\nimport java.util.HashMap\n\n// Create the Kafka producer\nval producerProperties = new HashMap[String, Object]()\nproducerProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokers)\nproducerProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n                           \"org.apache.kafka.common.serialization.StringSerializer\")\nproducerProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n                           \"org.apache.kafka.common.serialization.StringSerializer\")\nval producer = new KafkaProducer[String, String](producerProperties)\n\n// Iterate over data and emit to Kafka\njsonDataArray.foreach { row =>\n                // Get the row as a JSON string\n                val jsonData = gson.toJson(row)\n                // Create the message for Kafka\n                val message = new ProducerRecord[String, String](kafkaTopic, null, jsonData)\n                // Send the message\n                producer.send(message)\n                // Sleep a bit between sends to simulate streaming data\n                Thread.sleep(1000)\n             }\nproducer.close()\nprintln(\"Finished writting to Kafka\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Load and run the Stream-data-from-Kafka-to-Cosmos-DB notebook\n\nWhile the previous cell is active, load the other notebook in this project (Stream-data-from-Kafka-to-Cosmos-DB) and follow the steps in it.", "cell_type": "markdown", "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}