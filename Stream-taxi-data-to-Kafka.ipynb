{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Retrive taxi trip data for New York city\n",
    "\n",
    "This notebook retrieves data on taxi trips, which is provided by the New York City. The data is stored into Kafka, and then used by the other notebook in this project (Stream-data-from-Kafka-to-Cosmos-DB) to demonstrate how to write data into Cosmos.\n",
    "\n",
    "The data set used by this notebook is from [2016 Green Taxi Trip Data](https://data.cityofnewyork.us/Transportation/2016-Green-Taxi-Trip-Data/hvrh-b6nb).\n",
    "\n",
    "## To use this notebook\n",
    "\n",
    "Jupyter Notebooks allow you to modify and run the code in this document. To run a section (known as a 'cell',) select it and then use CTRL + ENTER, or select the play button on the toolbar above. Note that each section already has some example output beneath it, so you can see what the results of running a cell will look like.\n",
    "\n",
    "NOTE: You must run each cell in order, from top to bottom. Running cells out of order can result in an error.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* An Azure Virtual Network\n",
    "* A Spark (2.4) on HDInsight 4.0 cluster, inside the virtual network\n",
    "* A Kafka on HDInsight 4.0 cluster, inside the virtual network\n",
    "\n",
    "## Load packages\n",
    "\n",
    "Run the next cell to load packages used by this notebook:\n",
    "\n",
    "* spark-streaming-kafka-0-8_2.10, version 2.2.0 - Used to write data to Kafka.\n",
    "* gson version 2.4 - Used for JSON parsing.\n",
    "\n",
    "__NOTE__: The first time you run this block, it may take a minute or longer. This happens because the Spark cluster must retrieve the packages from the Maven repository on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-streaming-kafka-0-8_2.10:2.2.0,com.google.code.gson:gson:2.4\",\n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the Kafka topic\n",
    "\n",
    "In the next cell, you must provide the Zookeeper host information for your Kafka cluster. Use the following steps to get this information:\n",
    "\n",
    "* From __Bash__ or other Unix shell:\n",
    "\n",
    "    ```bash\n",
    "CLUSTERNAME='the name of your HDInsight cluster'\n",
    "PASSWORD='the password for your cluster login account'\n",
    "curl -u admin:$PASSWORD -G https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/ZOOKEEPER/components/ZOOKEEPER_SERVER| grep -i host_name | awk 'NR==1{print $NF,\":2181\"}'|tr -d '\"'|tr -d ' '\n",
    "    ```\n",
    "\n",
    "* From __Azure PowerShell__:\n",
    "\n",
    "    ```powershell\n",
    "$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n",
    "$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n",
    "$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/ZOOKEEPER/components/ZOOKEEPER_SERVER\" `\n",
    "    -Credential $creds `\n",
    "    -UseBasicParsing\n",
    "$respObj = ConvertFrom-Json $resp.Content\n",
    "$zkHosts = $respObj.host_components.HostRoles.host_name[0..1]\n",
    "($zkHosts -join \":2181,\") + \":2181\"\n",
    "    ````\n",
    "\n",
    "The return value is similar to the following example:\n",
    "\n",
    "`zk0-kafka.ztgnbfvxu2mudoa5h5zzc1uncg.cx.internal.cloudapp.net:2181,zk1-kafka.ztgnbfvxu2mudoa5h5zzc1uncg.cx.internal.cloudapp.net:2181`\n",
    "\n",
    "Replace the `YOUR_ZOOKEEPER_HOSTS` in the next cell with the returned value, and then run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic tripdata --zookeeper 'YOUR_ZOOKEEPER_HOSTS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Retrieve data on taxi trips\n",
    "\n",
    "Run the next cell to load data on taxi trips in New York City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Load the data from the New York City Taxi data REST API for 2016 Green Taxi Trip Data\n",
    "val url=\"https://data.cityofnewyork.us/resource/pqfs-mqru.json\"\n",
    "val result = scala.io.Source.fromURL(url).mkString\n",
    "\n",
    "// Since the REST API returns an array of items,\n",
    "// it's easier to use as an array than deal with streaming\n",
    "import com.google.gson.Gson\n",
    "val gson = new Gson()\n",
    "val jsonDataArray = gson.fromJson(result, classOf[Array[Object]])\n",
    "\n",
    "println(\"Retrieved \" + jsonDataArray.length + \" rows of Taxi data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Set the Kafka broker hosts information\n",
    "\n",
    "In the next cell, replace YOUR_KAFKA_BROKER_HOSTS with the broker hosts for your Kafka cluster. This is used to write data to the Kafka cluster. To get the broker host information, use one of the following methods:\n",
    "\n",
    "* From Bash or other Unix shell:\n",
    "\n",
    "    ```bash\n",
    "CLUSTERNAME='the name of your HDInsight cluster'\n",
    "PASSWORD='the password for your cluster login account'\n",
    "curl -u admin:$PASSWORD -G https://$CLUSTERNAME.azurehdinsight.net/api/v1/clusters/$CLUSTERNAME/services/KAFKA/components/KAFKA_BROKER| grep -i host_name | awk 'NR==1{print $NF,\":9092\"}'|tr -d '\"'|tr -d ' '\n",
    "    ```\n",
    "\n",
    "* From Azure Powershell:\n",
    "\n",
    "    ```powershell\n",
    "$creds = Get-Credential -UserName \"admin\" -Message \"Enter the HDInsight login\"\n",
    "$clusterName = Read-Host -Prompt \"Enter the Kafka cluster name\"\n",
    "$resp = Invoke-WebRequest -Uri \"https://$clusterName.azurehdinsight.net/api/v1/clusters/$clusterName/services/KAFKA/components/KAFKA_BROKER\" `\n",
    "  -Credential $creds `\n",
    "  -UseBasicParsing\n",
    "$respObj = ConvertFrom-Json $resp.Content\n",
    "$brokerHosts = $respObj.host_components.HostRoles.host_name[0..1]\n",
    "($brokerHosts -join \":9092,\") + \":9092\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// The Kafka broker hosts and topic used to write to Kafka\n",
    "val kafkaBrokers=\"YOUR_BROKER_HOSTS\"\n",
    "val kafkaTopic=\"tripdata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Send the data to Kafka\n",
    "\n",
    "Run the following cell to begin streaming data to Kafka. There is a delay of 1 second (1000ms) after each send, so this cell will stay active several minutes. This provides you the time needed to load the other notebook and run the cells in it while data is flowing into Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "// Import classes used to write to Kafka via a producer\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import java.util.HashMap\n",
    "\n",
    "// Create the Kafka producer\n",
    "val producerProperties = new HashMap[String, Object]()\n",
    "producerProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokers)\n",
    "producerProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n",
    "                           \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "producerProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n",
    "                           \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "val producer = new KafkaProducer[String, String](producerProperties)\n",
    "\n",
    "// Iterate over data and emit to Kafka\n",
    "jsonDataArray.foreach { row =>\n",
    "                // Get the row as a JSON string\n",
    "                val jsonData = gson.toJson(row)\n",
    "                // Create the message for Kafka\n",
    "                val message = new ProducerRecord[String, String](kafkaTopic, null, jsonData)\n",
    "                // Send the message\n",
    "                producer.send(message)\n",
    "                // Sleep a bit between sends to simulate streaming data\n",
    "                Thread.sleep(1000)\n",
    "             }\n",
    "producer.close()\n",
    "println(\"Finished writting to Kafka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and run the Stream-data-from-Kafka-to-Cosmos-DB notebook\n",
    "\n",
    "While the previous cell is active, load the other notebook in this project (Stream-data-from-Kafka-to-Cosmos-DB) and follow the steps in it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark | Scala",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
